{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXkbAByIv1gW+wgc2K3/vs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tejeswini-98/Apache-Spark-And-Hadoop/blob/main/Spark_Python_DAY_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "469YGYpWetWv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.we will be starting with spark,we will try to integrate python with spark\n",
        "2.We are going to use pyspark for doing some data analysis, we will be working on some machine learning models in pyspark\n",
        "3.Handoop in to work on aws(amzon web service)\n",
        "4.we will be creating accounts in aws(free tier accounts),you should have credit and debit card.\n",
        "5.Work on server there is ec2 instances(elastic cloud compute) there we will be launching our linux,install Handoop,setting up the configuration,then will be discussing small examples using map reduce concepts in Handoop.\n",
        "6.In the background we will be using small examples of programs written and java and then we are going to apply them on data.\n",
        "7.Word count,mean of words present in a text calculate the pi value examples.\n",
        "8.Databricks,platform where you can directly lauch your spark + python kernel and you can start working using the pyspark concepts.\n",
        "9.Just create one free account,we can start working(no need of having any credit card and debit card). "
      ],
      "metadata": {
        "id": "E2ei3wZIeubp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oiY5Y5SeiaTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark documentation\n",
        "https://spark.apache.org/docs/latest/api/python/\n",
        "\n",
        "Pyspark is a spark library written in python to run python applications using the Apache spark Capabilites,using pyspark we can run applications parallely on the distributed cluster(mulitiple nodes).\n",
        "*  python data frame is same used\n",
        "* spark working on streamy data -continous pyspark concepts.(twiiter continous data)\n",
        "\n",
        "In other words,pyspark is a python api(application programming inferface) for Apache Spark .integrate -spark start writing program in python(java,scala,r)\n",
        "pyspark  is used for huge data .dedicates some amount of data differnt cluster nodes  nodes nodes nodes.processses ur data quickly.\n",
        "Apache spark is an analytical processing engine for large scale powerful distributed data processing and machine learning applications.\n",
        "\n",
        "in real-time pyspark has used a lot in the machine  learning & Data science community. thanks to vast python machine learning libraries  \n",
        "* R ,python =ml models=pyspark =spark ml models -data (project on pyspark)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fjPlPooPngq5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python +spark =Pyspark.\n",
        "python=pyspark+ml model.\n",
        "google colab setuping the spark environment.\n",
        "we use handoop on aws(cloud prenium)\n",
        "using some of the aws service (free)\n",
        "lauch the services ec2  servce-linux-os(operating system)\n",
        "linux-java-installing(handoop and spark) for purposes -setup the configuration \n",
        "install handoop(same os linux)\n",
        "take examples-java-map reduce problems solve.\n",
        "text files ,countwords,avg length of words,medium length of words\n",
        "basically analysis the big data we are using Handoop.\n",
        "\n",
        "Data bricks: lauch notenoobook(jypter notebook) free"
      ],
      "metadata": {
        "id": "y8AZPK9Kifcm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Advantage of pyspark.\n",
        "* Pyspark is a general -purpose, in memory ,distributed processing engine that allows you to process data effieciently in a distributed fashions.\n",
        "* Application running onpyspark are 100x faster than traditional syatems.\n",
        "* you will get great benefits using pyspark for data ingestion pipelines.\n",
        "* using pyspark we can process data from Handoop HDFS,aws s3 and many others file systems.(handoop distributed storage files,aws s3,azurblob)\n",
        "\n",
        "* Pyspark also is used to process real -time data using streaming(continous twitter,instagram) and kafka.\n",
        "* Using pyspark stramming you can also streamfiles from the file systems and also stram for the socket.\n",
        "* Pyspark natively has marketing learning and grap libraires.\n",
        "* handoop is not capable of stramming data"
      ],
      "metadata": {
        "id": "_YfFkFvpr3xL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5khisWejxf-U"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oe1LjeKSxnm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pyspark modules & packages\n",
        "\n",
        "* pyspark RDD(pyspark RDD)- working on ur distributed data.\n",
        "* pyspark Dataframe and sql(pyspark sql application sql modules)\n",
        "*pyspark  Streamming(pyspark streaming)\n",
        "* pyspark  MLib( pyspark ml,pyspark,millib) for machine learning\n",
        "* pyspark GraphFrames(GrapFrames) graphs - networking analytics "
      ],
      "metadata": {
        "id": "CQ5jb0sHxpPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Zb03oKKxq1M",
        "outputId": "00c10d32-2250-4f7b-c9f6-750f00fad761"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Pyspark\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 48 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 50.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: Pyspark\n",
            "  Building wheel for Pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=42e81521ba3e500398a63eaa27b4c910624b47a35de309e43f55e30e8836a69b\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
            "Successfully built Pyspark\n",
            "Installing collected packages: py4j, Pyspark\n",
            "Successfully installed Pyspark-3.3.0 py4j-0.10.9.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zE0kFBtryCBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "APache Spark Eco System\n",
        "\n",
        "Spark,\n",
        "Spark streaming, \n",
        "Spark Mlib,\n",
        "Spark Graphx(network)\n",
        "\n",
        "Apache Spark Core ApI\n",
        "r,python,java,scala,sql"
      ],
      "metadata": {
        "id": "x4TVzNV2y6Ok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "-vp890MWzXyY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder\\\n",
        "      .master(\"local[1]\")\\\n",
        "      .appName(\"SparkByExamples.com\")\\\n",
        "      .getOrCreate()   "
      ],
      "metadata": {
        "id": "BapasWiJzuuq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BDINQQGg06tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://sparkbyexamples.com/\n",
        "\n",
        ". Spark SQL data frame \n",
        ". Streaming\n",
        ". MLib machine learning\n",
        ". Spark Core\n",
        "\n",
        "Spark Core\n",
        "\n",
        "Spark Core is the underlying general execution engine for the Spark platform that all other functionality is built on top of. It provides an RDD (Resilient Distributed Dataset) and in-memory computing capabilities.\n",
        "(some data processing using RDD)"
      ],
      "metadata": {
        "id": "vS-G7tDI1bP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datalist=[(\"java\",20000),(\"python\",10000),(\"scale\",3000)]"
      ],
      "metadata": {
        "id": "CuWcpP2j4U8j"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datalist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ma4B-xyr1cUW",
        "outputId": "0ea79d8a-c401-46c5-e51d-284dad2f8eeb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('java', 20000), ('python', 10000), ('scale', 3000)]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd=spark.sparkContext.parallelize(datalist)"
      ],
      "metadata": {
        "id": "ziic6bny5IXg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paralleliza()\n",
        "\n",
        "convert into RDD so we used the distribution nodes\n",
        "\n",
        "some small functions count,"
      ],
      "metadata": {
        "id": "xNoWaRZi6JAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRUMRJa86iIH",
        "outputId": "ee95ee4d-5456-4903-e4e5-3552eea3d70c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.collect()# view function in R "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apIA6i6E69Pw",
        "outputId": "e405e11a-440c-438b-cd37-3cb3b398f317"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('java', 20000), ('python', 10000), ('scale', 3000)]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.first()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0p5oxYBA9IoX",
        "outputId": "6e349918-e774-419b-b5f9-e71565edfa7f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('java', 20000)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.max()# which the last nodes rddis a data frame as in python rddd.max(keyvalue 0,1,2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7hhk_od-pbs",
        "outputId": "8b44922a-1b84-46d5-adbd-c1665a72cded"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('scale', 3000)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.min()# which the first node"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqcshdYK_ISi",
        "outputId": "787f7fe6-465d-4290-bd9c-d0668f8e79ba"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('java', 20000)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd=spark.sparkContext.parallelize([1.0,5.0,43.0,10.0])\n",
        "rdd.max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUO46mB-_qRe",
        "outputId": "35f45a99-a2b8-4f22-dab6-337979a99add"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "43.0"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [('James','','Smith','1991-04-01','M',3000),\n",
        "  ('Michael','Rose','','2000-05-19','M',4000),\n",
        "  ('Robert','','Williams','1978-09-05','M',4000),\n",
        "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
        "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
        "]\n",
        "columns = (\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\")\n",
        "df = spark.createDataFrame(data=data, schema = columns)"
      ],
      "metadata": {
        "id": "lC_R5RUXBUDV"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()# schema means layouts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jz7TWxpBCh1c",
        "outputId": "16d7e4c3-d493-42ed-881e-68d7b92a0f3e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+--------+----------+------+------+\n",
            "|firstname|middlename|lastname|       dob|gender|salary|\n",
            "+---------+----------+--------+----------+------+------+\n",
            "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
            "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
            "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
            "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
            "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
            "+---------+----------+--------+----------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "columns=[\"Language\",\"user_count\"]\n",
        "data=[('java','20000'),('python','100000'),('scala','3000')]"
      ],
      "metadata": {
        "id": "YUXtvz4-DKRD"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd=spark.sparkContext.parallelize(data)"
      ],
      "metadata": {
        "id": "SeLMwZNWFREP"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfFromRDD1=rdd.toDF()\n"
      ],
      "metadata": {
        "id": "nMaPO1iYFXLJ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfFromRDD1.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pv7UuklAF_-9",
        "outputId": "281f21c1-3d3a-4a1f-da92-e18a233e00ee"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _1: string (nullable = true)\n",
            " |-- _2: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " dfFromRDD1=rdd.toDF(columns)"
      ],
      "metadata": {
        "id": "8dhClcV2GGWF"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfFromRDD1.printSchema()# dataframe rddto dataframe build ml application."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cl-aGxcHC1c",
        "outputId": "d455b346-ec3c-49a4-e41f-b1a45cf2d30e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Language: string (nullable = true)\n",
            " |-- user_count: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DfFrameRDD2=spark.createDataFrame(rdd).toDF(*columns)"
      ],
      "metadata": {
        "id": "LeQEz88jHJdC"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfFromRDD1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WaBUMJ0ILaX",
        "outputId": "85701709-b5ae-4a9b-b8a8-f2a07a67be8c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+\n",
            "|Language|user_count|\n",
            "+--------+----------+\n",
            "|    java|     20000|\n",
            "|  python|    100000|\n",
            "|   scala|      3000|\n",
            "+--------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType,StructField,StringType,IntegerType\n",
        "data2 = [('James','','Smith','1991-04-01','M',3000),\n",
        "  ('Michael','Rose','','2000-05-19','M',4000),\n",
        "  ('Robert','','Williams','1978-09-05','M',4000),\n",
        "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
        "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
        "]\n",
        "schema=StructType([ \\\n",
        "    StructField(\"FirstName\",StringType(),True), \\\n",
        "    StructField(\"MiddleName\",StringType(),True), \\\n",
        "    StructField(\"lastName\",StringType(),True), \\\n",
        "    StructField(\"dob\",StringType(),True), \\\n",
        "    StructField(\"Gender\",StringType(),True), \\\n",
        "    StructField(\"Salary\",IntegerType(),True)          \n",
        "  ])\n",
        "df = spark.createDataFrame(data=data2, schema = schema)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xkLwc37IiEK",
        "outputId": "164e11ad-8708-46e5-9ab6-94710958064a"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- FirstName: string (nullable = true)\n",
            " |-- MiddleName: string (nullable = true)\n",
            " |-- lastName: string (nullable = true)\n",
            " |-- dob: string (nullable = true)\n",
            " |-- Gender: string (nullable = true)\n",
            " |-- Salary: integer (nullable = true)\n",
            "\n",
            "+---------+----------+--------+----------+------+------+\n",
            "|FirstName|MiddleName|lastName|dob       |Gender|Salary|\n",
            "+---------+----------+--------+----------+------+------+\n",
            "|James    |          |Smith   |1991-04-01|M     |3000  |\n",
            "|Michael  |Rose      |        |2000-05-19|M     |4000  |\n",
            "|Robert   |          |Williams|1978-09-05|M     |4000  |\n",
            "|Maria    |Anne      |Jones   |1967-12-01|F     |4000  |\n",
            "|Jen      |Mary      |Brown   |1980-02-17|F     |-1    |\n",
            "+---------+----------+--------+----------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRUNCATE= TRIM DATA\n"
      ],
      "metadata": {
        "id": "C9-PGSmaK_GM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UxHAjpu3LFFm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}